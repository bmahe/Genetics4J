= Mixture Models on GPU
:stem:
:toc:
:docinfo:
:sourcedir: ../../../src/main/java
:outdir: ../../target/site
:icons: font

== Preface

The purpose of this document is to serve me as notes and is not meant to be taken as exhaustive or a fully fledged blog post.


== Introduction

link:https://en.wikipedia.org/wiki/Mixture_model[Mixture models] are a probabilistic view of the data, by decomposing it across different probility distributions.

In this example, we will use a _Genetic Algorithm_ based approach for the detemination of the best mixture of probility distributions for a set of 2D points. We will focus on normal distributions only. As we are looking at a potentially large set of items, doing such computation on CPU may take too much time. Thus we will also look at doing these computations on a GPU and the associated challenges.

Solving that challenge means we will need a few things:

* We need to generate a dataset that we can cluster
* We need to figure out a good way to represent our population of clusters, which are our solutions
* We need to establish a way to grade our clusters, which is our fitness evaluation function
* We will also have to configure our evolution with some sensible policies for selection, combination and mutation
* We will need to modify our algorithms so they can be run on a GPU and interface with them


=== What is a Mixture Model?

As the name implies, it assumes the data is generated from a model stem:[ccM] of a mixture of _k_ probability distributions. This can be expressed as:

stem:[f^"point"(bar X_j|ccM) = sum_{i=0}^{k-1}alpha_i*f^i(bar X_j)]

with stem:[sum_{i=0}^{k-1}alpha_i = 1]

Where:

* stem:[ccM] represents the whole model, ie. the combination of these mixture components
* stem:[bar X_j] represents the stem:[j^{th}] data point out of the stem:[j in {1...n}] points in the dataset
* stem:[alpha_i] represents the prior probability for a given component
* stem:[f^i(bar X_j)] is the stem:[i^{th}] probability density function

Then if we generalize it to a dataset stem:[ccD] of points stem:[bar X_0...bar X_{n-1}], the probability density of this data being generated by that model stem:[ccM] is the product of probability density of each of these points:

stem:[f^{data}(ccD|ccM) = prod_{j=0}^{n-1}f^{"point"}(bar X_j|ccM)]

The goal will then be about finding the model stem:[ccM] which explains the best the dataset stem:[ccD], and thus maximizes stem:[f^{data}(ccD|ccM)]. However it is often easier to optimize for stem:[log(f^{data}(ccD|ccM))] since it transforms the product into a sum and reduces possible underflow and overflows computations.

This means we will now try to optimize:

stem:[log(f^{data}(ccD|ccM)) = sum_{j=0}^{n-1}log(f^{"point"}(bar X_j|ccM))	]

Effectively, this means our Genetic Algorithm will explore populations of various number of distributions and parameters and derive their fitness based on how high their stem:[log(f^{data}(ccD|ccM))] is. There will be some interesting challenges as we strive to find the inviduals which explain the most data with the least amount of distributions.

== Project setup

In order to do our clustering, we do need a dataset. To that end, we will generate multiple groups of 2D points.

The dataset is generated by first generating some random centroids and leveraging link:https://commons.apache.org/proper/commons-math/index.html[Apache Commons Math] to generate some samples:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/Main.java[tags=data_generation]
----


Here is how the resulting data would look like:

image::mixturemodel/original.png[Data plot]

Note how some of the clusters are really close to each others and somewhat overlapping. This will have an impact in the way the algorithms decide to cluster them and it will be interesting to observe how each algorithm decides to approach that ambiguity.


=== Baseline - Expectation-Maximization

We want to compare our results with the ones produced by the most popular algorithm to estimate the parameters of a mixture model: link:https://en.wikipedia.org/wiki/Expectation-maximization_algorithm[Expectation-Maximization]. As it is pretty similar to link:https://en.wikipedia.org/wiki/K-means_clustering[K-means], it suffers from the same issue converging towards a local optimum, and not necessarily the global optimum.

link:https://commons.apache.org/proper/commons-math/index.html[Apache Commons Math] provide such support and we will use it to compute our baseline:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/Main.java[tags=commons-math]
----

Here is the output of such execution:

image::mixturemodel/assigned-commons-math.png[Data plot]



== Evolutionary Algorithm

=== Representation

If we expand the function we want to optimize for, we can write:

stem:[log(f^{data}(ccD|ccM)) = sum_{j=0}^{n-1}log(f^{"point"}(bar X_j|ccM)) = sum_{j=0}^{n-1}log(sum_{i=0}^{k-1}alpha_i*f^i(bar X_j))]

And since we are limiting ourselves to mixtures composed of Normal distributions, we can assume:

stem:[f^i(bar X_j) = 1/(2*pi*sqrt(|Sigma|))e^(-1/2*(x-mu)^T*Sigma^-1*(x-mu))]

Where:

* stem:[mu] is the mean vector for that distribution
* stem:[Sigma] is the covariance matrix stem:[[[sigma_x,sigma_(xy)\],[sigma_(xy),sigma_y\]\]]

This means we will nave _n_ mixtures and each one will require a total of 6 pieces of information:

* stem:[alpha_i] which is a scalar
* stem:[mu] which is a 2d vector
* stem:[Sigma] which is the covariance matrix. Since the covariance matrix for a normal distribution is a symmetric positive semi-definite matrix, we will only require three elements stem:[sigma_x, sigma_y, sigma_(xy)]
* stem:[|Sigma|] is the determinant of the covariance matrix
* stem:[Sigma^-1] is the inverse of the covariance matrix

As a result, we will use a link:../apidocs/net/bmahe/genetics4j/core/spec/chromosome/DoubleChromosomeSpec.html[DoubleChromosomeSpec] or link:../apidocs/net/bmahe/genetics4j/core/spec/chromosome/FloatChromosomeSpec.html[FloatChromosomeSpec] with values ranging 0 to 30, which is the range of the generated distributions. We will add an extra translation step for the covariances since their values could also be negative.

Alternatively, we could have use values between 0 and 1 and apply translation and scaling as appropriate.


=== First attempt - Single Objective Method

Since our genotype describes a model with the mixtures and their parameter, the fitness function can bluntly compute stem:[log(f^{data}(ccD|ccM))]. Since it is a _double_, the genetic algorithm will try to optimize that and we should end up with some interesting results.
Here is the definition of fitness function:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/SingleObjectiveMethod.java[tags=som_fitness]
----

Note it may be tempting to use a link:https://en.wikipedia.org/wiki/Silhouette_(clustering)[Silhouette score] like we did in link:./clustering.html[Clustering], but that would not too well since our mixtures aren't globular and thus could not fit well. However that's because we were using an euclidian distance. It may be interesting to investigate the use of the _Silhouette score_ in conjunction with a link:https://en.wikipedia.org/wiki/Mahalanobis_distance[Mahalanobis distance], which would be appropriate if the underlying distributions are assumed to be _gaussian_.


With regards to the configuration of the genetic algorithm, we will use a combination of creep, random and swap mutations, which will help explore the search space along with looking for better values.

The code for the configuration is:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/SingleObjectiveMethod.java[tags=som_config]
----

The execution context is also straightforward with a population of 250 individuals:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/SingleObjectiveMethod.java[tags=som_eaexeccontext]
----


==== Results

Here is an example of evolution of the fitness over time:

image::mixturemodel/mixturemodel-so-cpu.png[Data plot]

And if we cut off the first 100 generations, we can better observe the slow increase and could reasonably expect it to reach the same or better score than the baseline with enough generations:

image::mixturemodel/mixturemodel-so-cpu-s100.png[Data plot]


With regards to the clustering itself, while the clusters are pretty well defined, we do observe some surprising results for the centroids:

image::mixturemodel/assigned-so-5.png[Data plot]

We observe the clustering looks overall reasonable, except for the lower left where there appears to be an extra cluster and the boundaries look a bit off. It would be interesting to weigh in the impact of the number of clusters. We could look at link:https://en.wikipedia.org/wiki/Akaike_information_criterion[AIC] or link:https://en.wikipedia.org/wiki/Bayesian_information_criterion[BIC] which incorporate the complexity of the model with its likelihood. However let's try to evaluate them separately.


=== Second attempt - Multi Objective Optimization

One way to look at the problem is that there is a tradeof between the best fit and the number of clusters. So we could translate it in a multi objective optimization problem where we can express such trade off.

The fitness function would now return a vector with two components:

* The likelihood of such model, as in the previous section
* The number of unused clusters

The fitness function would be pretty similar to the previous one:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooCPU.java[tags=moo_cpu_fitness]
----

With regards to the configuration, except for the NSGA2 part, it is pretty much the same:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooCPU.java[tags=moo_cpu_config]
----

And the same can be said about its execution context:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooCPU.java[tags=moo_cpu_eaexeccontext]
----

==== Results

Let's look at the results for the best likelyhood across the number of clusters:

image::mixturemodel/assigned-moo-cpu-1.png[Data plot]

image::mixturemodel/assigned-moo-cpu-2.png[Data plot]

image::mixturemodel/assigned-moo-cpu-3.png[Data plot]

image::mixturemodel/assigned-moo-cpu-4.png[Data plot]

image::mixturemodel/assigned-moo-cpu-5.png[Data plot]


If we look at the overall fitness, we can observe a similar behavior than before:

image::mixturemodel/mixturemodel-moo-cpu.png[Data plot]

And if we cut off the first 100 generations, we can better observe the slow increase and could reasonably expect it to reach the same or better score than the baseline with enough generations:

image::mixturemodel/mixturemodel-moo-cpu-s100.png[Data plot]


This looks great, however it does take some time to compute. In order to accelerate the process, let's now look at accelerating this with the help of a GPU


=== Third attempt - Multi Objective Optimization over GPU


Leveraging GPUs can be quite a daunting task as they can be complex to interact with. Fortunately, Genetics4j does provide a few facilities to make it easier and safer to use OpenCL for hardware acceleration. Let's go through a cursory review of them.

==== GPUEAConfiguration

First, let's observe the configuration itself has not changed much comparing to the previous approaches:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooGPU.java[tags=moo_gpu_config]
----

The main differences we can observe are:

* We are now using _GPUEAConfiguration_ instead of a _EAConfiguration_
* We have a new _program_ definition to inform Genetics4j
* There is a new way to specify how to define the fitness


SingleKernelFitness is a way to specify two things:

* SingleKernelFitnessDescriptor - How to call the OpenCL clusters, including some contextual information such as the work size as well as the parameters to pass to said kernel
* FitnessExtractor - How to massage the result of the execution of the kernel into the actual fitness metric

==== SingleKernelFitnessDescriptor

Here is how we use it:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooGPU.java[tags=moo_gpu_skfd]
----

We can observe it specifies a few information with regards to which kernel to execute and how, but also how to prepare the parameters and data to pass to the kernel. There are actually four types of parameters Genetics4j offers in its abstraction:

* Static data, which is data that never changes across the genotype and generation. This is helpful to specify some side data or global parameters
* Dynamic data, which can depend on the specific genotypes and generation. This may contains the population, some related metadata or the result of some population specific computation
* Local memory, which can be used as temporary buffers for the computations done on the GPU
* Result allocators, which are buffers allocated to store the results and compute the final fitness score


==== Fitness Extractor

Here is how we define it:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooGPU.java[tags=moo_gpu_fitness_extractor]
----

The interesting part is how we use the _resultExtractor_ to extract the results of the OpenCL kernel to transform it into a _FitnessVector<Float>>_.

==== GPUEAExecutionContext

Here is how we define it:

[source,java,indent=0]
----
include::{sourcedir}/net/bmahe/genetics4j/samples/mixturemodel/MooGPU.java[tags=moo_gpu_eaexeccontext]
----

It is used in the very same way than a regular EAExecutionContext, except for additional helpers to finely control which device(s) are to be used. The evolution will take advantage of all the selected devices, significantly increasing the computation speed in the case of multiple GPUs or hardware accelerators.

==== Results

Looking at the logs, we can confirm Genetics4j is leveraging all the GPUs devices it could find on the machine:

[source,bash]
----
01:23:31.936 DEBUG n.b.g.g.GPUFitnessEvaluator - Genotype decomposed in 2 partition(s)
01:23:31.948 DEBUG n.b.g.g.s.f.SingleKernelFitness - Intel(R) Graphics [0x9a60] - Took 33.047 microsec for 250 genotypes
01:23:31.975 DEBUG n.b.g.g.s.f.SingleKernelFitness - NVIDIA GeForce RTX 3060 Laptop GPU - Took 1366.255 microsec for 250 genotypes
----

The increased execution speed means we can run algorithms on larger populations, for larger dataset and for far more generations!


Then let's look at the results for the best likelyhood across the number of clusters:

image::mixturemodel/assigned-mixturemodel-moo-gpu-1.png[Data plot]

image::mixturemodel/assigned-mixturemodel-moo-gpu-2.png[Data plot]

image::mixturemodel/assigned-mixturemodel-moo-gpu-3.png[Data plot]

image::mixturemodel/assigned-mixturemodel-moo-gpu-4.png[Data plot]

image::mixturemodel/assigned-mixturemodel-moo-gpu-5.png[Data plot]


If we look at the overall fitness, we can observe a similar behavior than before:

image::mixturemodel/mixturemodel-moo-gpu.png[Data plot]

And if we cut off the first 200 generations, we can better observe the slow increase and could reasonably expect it to reach the same or better score than the baseline with enough generations:

image::mixturemodel/mixturemodel-moo-gpu-s100.png[Data plot]



== Conclusion

Genetic Algorithms can be pretty effective at clustering data without making some assumption about the underlying dataset. This can be especially be useful in more complex scenario. We also observe that the increase in parameters will also increase the time to find more acceptable solution as the solution space is much larger.

Leveraging GPUs, we are also able to work on more complex challenges in a more reasonable time, enabling faster iterations and broader exploration of the solution space.


== Going further

This is a rather simple example but there are tons of room for improvement.

* We have not looked at incorporating AIC or BIC into the scoring methodology
* The way we compute the fitness score is far from being optimized, whether the CPU version or the GPU one
* Instead of assuming centroids have a specific location, we could consider them instead as statistical distributions and thus a link:https://en.wikipedia.org/wiki/Mixture_model[Mixture model]
* We have not explored _Fitness Sharing_

